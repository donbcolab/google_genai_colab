{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMr4SC/8t9aiuITDbYsCbaC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donbcolab/google_genai_colab/blob/main/gemini_multi_modal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemini Multi Modal app - leveraging Gradio\n",
        "\n",
        "- Inspired by the [Python Docs Samples](https://github.com/GoogleCloudPlatform/python-docs-samples) Repository\n",
        "  - The Python Docs Samples repository is amazing.  It provides some very concise and to the point examples to test and validate core Google Generative AI models.\n",
        "  - below are some simple ones that I wrapped in a simple gradio application\n",
        "- [A Better Way to Use Google Cloud from Colab](https://medium.com/google-colab/a-better-way-to-use-google-cloud-from-colab-bb93f88b5021) is a fantastic article by Laurent Picard that demystifies the authentication flow"
      ],
      "metadata": {
        "id": "4QyOREwCzcAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-requisistes\n",
        "\n",
        "- a Google Cloud Project setup with required APIs enabled in **us-central1**\n",
        "- other locations may be available after April 2024\n",
        "- For the current Notebook the key API is\n",
        "  - Vertex AI API (aiplatform.googleapis.com)"
      ],
      "metadata": {
        "id": "WC6CiX1a45rB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Project Setup and Authentication"
      ],
      "metadata": {
        "id": "NsJ_ImpeGJov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "PROJECT_ID = userdata.get('GOOGLE_CLOUD_PROJECT_ID')\n",
        "LOCATION = userdata.get('GOOGLE_CLOUD_LOCATION')"
      ],
      "metadata": {
        "id": "Acig4mxA2OuM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user(project_id=PROJECT_ID)"
      ],
      "metadata": {
        "id": "RiVK2PJB4XYH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Python dependencies"
      ],
      "metadata": {
        "id": "CvJkpZJuGQzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install -U -q google-cloud-aiplatform\n",
        "! pip3 install -U -q 'anthropic[vertex]'\n",
        "! pip install -q gradio"
      ],
      "metadata": {
        "id": "jSigteS_0Vvm",
        "outputId": "13617a6e-1f90-484d-dd94-18562ec6ff77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m877.7/877.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.9/315.9 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vertex and Gemini App Logic"
      ],
      "metadata": {
        "id": "B4uFdF2jF-F_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "import time\n",
        "from vertexai.generative_models import GenerativeModel, ChatSession, Part\n",
        "from vertexai.preview.vision_models import Image, ImageGenerationModel, ImageTextModel\n",
        "from anthropic import AnthropicVertex\n",
        "from vertexai.preview.generative_models import (\n",
        "    grounding,\n",
        "    Tool,\n",
        ")\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "def summarize_audio(audio_file_path):\n",
        "    model = GenerativeModel(\"gemini-1.5-pro-preview-0514\")\n",
        "    prompt = \"\"\"\n",
        "    Please provide a summary for the audio.\n",
        "    Provide chapter titles with timestamps, be concise and short, no need to provide chapter summaries.\n",
        "    Do not make up any information that is not part of the audio and do not be verbose.\n",
        "    \"\"\"\n",
        "    time.sleep(3)\n",
        "    print(audio_file_path)\n",
        "    with open(audio_file_path, \"rb\") as audio_file:\n",
        "        audio_bytes = audio_file.read()\n",
        "    audio = Part.from_data(audio_bytes, mime_type=\"audio/mpeg\")\n",
        "    contents = [audio, prompt]\n",
        "    response = model.generate_content(contents)\n",
        "    return response.text\n",
        "\n",
        "def transcript_audio(audio_file_path):\n",
        "    model = GenerativeModel(\"gemini-1.5-pro-preview-0514\")\n",
        "    prompt = \"\"\"\n",
        "    Transcribe this recording.\n",
        "    If there are multiple speakers capture it in the format of timecode, speaker, caption.\n",
        "    Use speaker A, speaker B, etc. to identify speakers.\n",
        "    \"\"\"\n",
        "    time.sleep(3)\n",
        "    print(audio_file_path)\n",
        "    with open(audio_file_path, \"rb\") as audio_file:\n",
        "        audio_bytes = audio_file.read()\n",
        "    audio = Part.from_data(audio_bytes, mime_type=\"audio/mpeg\")\n",
        "    contents = [audio, prompt]\n",
        "    response = model.generate_content(contents)\n",
        "    return response.text\n",
        "\n",
        "def analyze_video_with_audio(video_file_path):\n",
        "    model = GenerativeModel(\"gemini-1.5-pro-preview-0514\")\n",
        "    prompt = \"\"\"\n",
        "    Provide a description of the video.\n",
        "    The description should also highlight anything important which people say in the video.\n",
        "    \"\"\"\n",
        "    time.sleep(5)\n",
        "    print(video_file_path)\n",
        "    with open(video_file_path, \"rb\") as video_file:\n",
        "        video_bytes = video_file.read()\n",
        "    video = Part.from_data(video_bytes, mime_type=\"video/mp4\")\n",
        "    contents = [video, prompt]\n",
        "    response = model.generate_content(contents)\n",
        "    return response.text\n",
        "\n",
        "def generate_image(prompt):\n",
        "    model = ImageGenerationModel.from_pretrained(\"imagegeneration@006\")\n",
        "\n",
        "    aspect_ratio = \"1:1\"\n",
        "    safety_filter_level = \"block_few\"\n",
        "    person_generation = \"allow_adult\"\n",
        "\n",
        "    generate_response = model.generate_images(\n",
        "        prompt=prompt,\n",
        "        number_of_images=1,\n",
        "        language=\"en\",\n",
        "        aspect_ratio=aspect_ratio,\n",
        "        safety_filter_level=safety_filter_level,\n",
        "        person_generation=person_generation,\n",
        "    )\n",
        "\n",
        "    images = []\n",
        "    for index, result in enumerate(generate_response):\n",
        "        images.append(generate_response[index]._pil_image)\n",
        "    return images[0]\n",
        "\n",
        "def caption_image(input_file):\n",
        "    model = ImageTextModel.from_pretrained(\"imagetext@001\")\n",
        "    time.sleep(5)\n",
        "    print(input_file)\n",
        "    source_img = Image.load_from_file(location=input_file)\n",
        "\n",
        "    captions = model.get_captions(\n",
        "        image=source_img,\n",
        "        language=\"en\",\n",
        "        number_of_results=1,\n",
        "    )\n",
        "\n",
        "    return captions\n",
        "\n",
        "def generate_claude_text(prompt):\n",
        "    client = AnthropicVertex(project_id=PROJECT_ID, region=LOCATION)\n",
        "    result = []\n",
        "\n",
        "    with client.messages.stream(\n",
        "        model=\"claude-3-haiku@20240307\",\n",
        "        max_tokens=1024,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    ) as stream:\n",
        "        for text in stream.text_stream:\n",
        "            result.append(text)\n",
        "\n",
        "    return \"\".join(result)\n",
        "\n",
        "def generate_text_with_grounding(chat_prompt):\n",
        "    model = GenerativeModel(\"gemini-1.5-pro-preview-0514\")\n",
        "    chat = model.start_chat()\n",
        "\n",
        "    def get_chat_response(chat: ChatSession, prompt: str) -> str:\n",
        "        tool = Tool.from_google_search_retrieval(grounding.GoogleSearchRetrieval())\n",
        "\n",
        "        response = chat.send_message(prompt, tools=[tool])\n",
        "        return response.text\n",
        "\n",
        "    prompt = chat_prompt\n",
        "    return get_chat_response(chat, prompt)"
      ],
      "metadata": {
        "id": "Nj21bOSDzCYs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio Web App"
      ],
      "metadata": {
        "id": "bdgvWFARGcKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Gemini Multi Modal Generation and Analysis\")\n",
        "\n",
        "    with gr.Tab(\"Audio Summarization\"):\n",
        "        audio_file_summary = gr.Audio(sources=[\"microphone\", \"upload\"], type=\"filepath\", label=\"Upload Audio\")\n",
        "        summarize_button = gr.Button(\"Summarize Audio\")\n",
        "        summary_output = gr.Textbox(label=\"Audio Summary\")\n",
        "        summarize_button.click(\n",
        "            fn=summarize_audio,\n",
        "            inputs=audio_file_summary,\n",
        "            outputs=summary_output\n",
        "            )\n",
        "\n",
        "    with gr.Tab(\"Audio Transcription\"):\n",
        "        audio_file_transcript = gr.Audio(sources=[\"microphone\", \"upload\"], type=\"filepath\", label=\"Upload Audio\")\n",
        "        transcript_button = gr.Button(\"Transcribe Audio\")\n",
        "        transcript_output = gr.Textbox(label=\"Audio Transcription\")\n",
        "        transcript_button.click(\n",
        "            fn=transcript_audio,\n",
        "            inputs=audio_file_transcript,\n",
        "            outputs=transcript_output\n",
        "            )\n",
        "\n",
        "    with gr.Tab(\"Video Analysis\"):\n",
        "        video_file = gr.Video(label=\"Upload Video\", format=\"mp4\")\n",
        "        analyze_button = gr.Button(\"Analyze Video\")\n",
        "        analysis_output = gr.Textbox(label=\"Video Analysis\")\n",
        "        analyze_button.click(\n",
        "            fn=analyze_video_with_audio,\n",
        "            inputs=video_file,\n",
        "            outputs=analysis_output\n",
        "            )\n",
        "\n",
        "    with gr.Tab(\"Image Generation\"):\n",
        "        image_prompt = gr.Textbox(label=\"Image Prompt\")\n",
        "        generate_button = gr.Button(\"Generate Image\")\n",
        "        image_output = gr.Image(label=\"Generated Images\")\n",
        "        generate_button.click(\n",
        "            fn=generate_image,\n",
        "            inputs=image_prompt,\n",
        "            outputs=image_output\n",
        "            )\n",
        "\n",
        "    with gr.Tab(\"Image Captioning\"):\n",
        "        caption_prompt = gr.Image(label=\"Image\", type=\"filepath\")\n",
        "        caption_button = gr.Button(\"Generate Caption\")\n",
        "        caption_output = gr.Textbox(label=\"Image Caption\")\n",
        "        caption_button.click(\n",
        "            fn=caption_image,\n",
        "            inputs=caption_prompt,\n",
        "            outputs=caption_output\n",
        "            )\n",
        "\n",
        "    with gr.Tab(\"Anthropic Claude\"):\n",
        "        claude_prompt = gr.Textbox(label=\"Prompt\")\n",
        "        claude_button = gr.Button(\"Generate Text\")\n",
        "        claude_output = gr.Textbox(label=\"Generated Text\")\n",
        "        claude_button.click(\n",
        "            fn=generate_claude_text,\n",
        "            inputs=claude_prompt,\n",
        "            outputs=claude_output\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"Google Search as a Tool for Grounding\"):\n",
        "        grounding_prompt = gr.Textbox(label=\"Prompt\")\n",
        "        grounding_button = gr.Button(\"Generate Text\")\n",
        "        grounding_output = gr.Textbox(label=\"Generated Text\")\n",
        "        grounding_button.click(\n",
        "            fn=generate_text_with_grounding,\n",
        "            inputs=grounding_prompt,\n",
        "            outputs=grounding_output\n",
        "        )\n",
        "\n",
        "    demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "TccQtsPTzMKC",
        "outputId": "c76e2c74-b1c7-4289-e865-43be701f001a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://05c18c48300fc923ce.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://05c18c48300fc923ce.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}